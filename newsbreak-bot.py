#!/usr/bin/env python3
"""
Telegram Bot for NewsBreak Scraper Status Monitoring
Fetches status from NewsBreak API and sends updates to Telegram
Also integrates with ACK table to get accurate processing rates
"""

import asyncio
import aiohttp
import logging
from datetime import datetime, timedelta
import json
import os
from typing import Optional, Dict, Any
import asyncpg

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # dotenv not installed, will use system environment variables
    pass

# Simple async Telegram bot implementation
class TelegramBot:
    def __init__(self, bot_token: str, chat_id: str):
        self.bot_token = bot_token
        self.chat_id = chat_id
        self.base_url = f"https://api.telegram.org/bot{bot_token}"
        
    async def send_message(self, text: str, parse_mode: str = "HTML"):
        """Send message to Telegram chat"""
        url = f"{self.base_url}/sendMessage"
        payload = {
            "chat_id": self.chat_id,
            "text": text,
            "parse_mode": parse_mode
        }
        
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(url, json=payload) as response:
                    if response.status == 200:
                        return await response.json()
                    else:
                        logging.error(f"Telegram API error: {response.status}")
                        return None
            except Exception as e:
                logging.error(f"Error sending Telegram message: {e}")
                return None

class NewsBreakMonitor:
    def __init__(self, api_url: str, bot: TelegramBot, db_url_data: str, db_url_urls: str):
        self.api_url = api_url.rstrip('/')
        self.bot = bot
        self.db_url_data = db_url_data
        self.db_url_urls = db_url_urls
        
    async def fetch_status(self) -> Optional[dict]:
        """Fetch status from NewsBreak API"""
        url = f"{self.api_url}/api/v1/scraper/status"
        
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get('data', {})
                    else:
                        logging.error(f"API error: {response.status}")
                        return None
            except Exception as e:
                logging.error(f"Error fetching status: {e}")
                return None
    
    async def get_today_ack_processing_stats(self) -> Dict[str, Any]:
        """Get today's URL processing statistics from ACK table"""
        today = datetime.now().date()
        today_start = datetime.combine(today, datetime.min.time())
        today_end = datetime.combine(today, datetime.max.time())
        
        stats = {
            'urls_processed_today': 0,
            'processing_rate_per_hour': 0.0,
            'processing_rate_per_minute': 0.0,
            'total_processing_time': timedelta(0),
            'ack_data_points': 0
        }
        
        try:
            conn = await asyncpg.connect(self.db_url_data)
            try:
                # Get ACK data points for today
                rows = await conn.fetch(
                    """
                    SELECT ack_total, ack_rate_per_sec, timestamp
                    FROM ack 
                    WHERE timestamp >= $1 AND timestamp <= $2
                    ORDER BY timestamp ASC
                    """,
                    today_start, today_end
                )
                
                if rows:
                    stats['ack_data_points'] = len(rows)
                    
                    # Calculate processing difference from first to last record of today
                    if len(rows) >= 2:
                        first_record = rows[0]
                        last_record = rows[-1]
                        
                        ack_difference = last_record['ack_total'] - first_record['ack_total']
                        time_difference = last_record['timestamp'] - first_record['timestamp']
                        
                        stats['urls_processed_today'] = ack_difference
                        stats['total_processing_time'] = time_difference
                        
                        # Calculate rates
                        if time_difference.total_seconds() > 0:
                            stats['processing_rate_per_hour'] = (ack_difference / (time_difference.total_seconds() / 3600))
                            stats['processing_rate_per_minute'] = (ack_difference / (time_difference.total_seconds() / 60))
                    
                    # If only one record today, use the rate from that record
                    elif len(rows) == 1:
                        single_record = rows[0]
                        stats['urls_processed_today'] = 0  # Can't calculate difference with single point
                        stats['processing_rate_per_minute'] = single_record['ack_rate_per_sec'] * 60
                        stats['processing_rate_per_hour'] = single_record['ack_rate_per_sec'] * 3600
                
                logging.info(f"Retrieved {stats['ack_data_points']} ACK data points for today")
                
            finally:
                await conn.close()
                
        except Exception as e:
            logging.error(f"Error fetching today's ACK processing stats: {e}")
        
        return stats
    
    async def get_today_stats_from_db(self) -> dict:
        """Get today's statistics directly from database using created_at field"""
        today = datetime.now().date()
        today_start = datetime.combine(today, datetime.min.time())
        today_end = datetime.combine(today, datetime.max.time())
        
        stats = {
            'urls_collected_today': 0,
            'data_extracted_today': 0
        }
        
        try:
            # Get today's URLs from urls database
            conn_urls = await asyncpg.connect(self.db_url_urls)
            try:
                urls_today = await conn_urls.fetchval(
                    "SELECT COUNT(*) FROM urls WHERE created_at >= $1 AND created_at <= $2",
                    today_start, today_end
                )
                stats['urls_collected_today'] = urls_today or 0
            finally:
                await conn_urls.close()
            
            # Get today's data from newsbreak_data database
            conn_data = await asyncpg.connect(self.db_url_data)
            try:
                data_today = await conn_data.fetchval(
                    "SELECT COUNT(*) FROM newsbreak_data WHERE created_at >= $1 AND created_at <= $2",
                    today_start, today_end
                )
                stats['data_extracted_today'] = data_today or 0
            finally:
                await conn_data.close()
                
        except Exception as e:
            logging.error(f"Error fetching today's stats from database: {e}")
        
        return stats

    async def format_daily_report(self, status: dict) -> str:
        """Format daily status report with today's progress from database and ACK table"""
        now = datetime.now()
        today = now.strftime("%Y-%m-%d")
        time_str = now.strftime("%H:%M:%S")
        
        if not status:
            return f"ğŸ”´ <b>Daily NewsBreak Report</b>\nğŸ“… {today} {time_str}\nâŒ <b>API Unavailable</b>"
        
        scraper_status = status.get('status', 'unknown')
        stats = status.get('statistics', {})
        queue_sizes = stats.get('queue_sizes', {})
        
        # Current totals from API
        current_urls = stats.get('urls_collected', 0)
        current_data = stats.get('data_extracted', 0)
        current_errors = stats.get('errors_count', 0)
        
        # Calculate processed URLs: urls_collected - scraper_queue_size - dlx_queue_size
        scraper_queue_size = queue_sizes.get('scraper_queue', 0)
        dlx_queue_size = queue_sizes.get('dlx_queue', 0)
        current_processed_urls = current_urls - scraper_queue_size - dlx_queue_size
        
        # Get today's stats from database
        today_stats = await self.get_today_stats_from_db()
        today_urls = today_stats.get('urls_collected_today', 0)
        today_data = today_stats.get('data_extracted_today', 0)
        
        # Get today's processing stats from ACK table
        today_ack_stats = await self.get_today_ack_processing_stats()
        today_processed_urls = today_ack_stats.get('urls_processed_today', 0)
        processing_rate_per_hour = today_ack_stats.get('processing_rate_per_hour', 0.0)
        processing_rate_per_minute = today_ack_stats.get('processing_rate_per_minute', 0.0)
        ack_data_points = today_ack_stats.get('ack_data_points', 0)
        
        # Status emoji
        status_emoji = "ğŸŸ¢" if scraper_status == "running" else "ğŸ”´"
        
        message = f"{status_emoji} <b>Daily NewsBreak Report</b>\n"
        message += f"ğŸ“… <b>Date:</b> {today} {time_str}\n"
        message += f"ğŸ”§ <b>Status:</b> {scraper_status.upper()}\n"
        message += f"ğŸ‘¥ <b>Active Workers:</b> {stats.get('active_workers', 0)}\n\n"
        
        message += f"ğŸ“ˆ <b>Today's Progress:</b>\n"
        message += f"ğŸ”— URLs Collected: {today_urls:,}\n"
        message += f"ğŸŸ¢ URLs Processed: {today_processed_urls:,}\n"
        message += f"ğŸ“° Data Extracted: {today_data:,}\n\n"
        
        # Add ACK-based processing rates
        if ack_data_points > 0:
            message += f"âš¡ <b>Processing Performance (ACK Data):</b>\n"
            message += f"ğŸ“Š URLs per Hour: {processing_rate_per_hour:.1f}\n"
            message += f"âš¡ URLs per Minute: {processing_rate_per_minute:.2f}\n"
            message += f"ğŸ“ˆ ACK Data Points: {ack_data_points}\n\n"
        else:
            message += f"âš ï¸ <b>Processing Performance:</b>\n"
            message += f"ğŸ“Š No ACK data available for today\n\n"
        
        message += f"ğŸ“Š <b>Total Statistics:</b>\n"
        message += f"ğŸ”— Total URLs: {current_urls:,}\n"
        message += f"ğŸŸ¢ Total Processed: {current_processed_urls:,}\n"
        message += f"ğŸ“° Total Data: {current_data:,}\n"
        message += f"âŒ Total Errors: {current_errors:,}\n\n"
        
        message += f"ğŸ“‹ <b>Current Queue Status:</b>\n"
        message += f"ğŸ•·ï¸ Scraper Queue: {queue_sizes.get('scraper_queue', 0):,}\n"
        message += f"ğŸ”— URLs Queue: {queue_sizes.get('newsbreak_urls_queue', 0):,}\n"
        message += f"ğŸ“° Data Queue: {queue_sizes.get('newsbreak_data_queue', 0):,}\n"
        message += f"ğŸ’€ DLX Queue: {queue_sizes.get('dlx_queue', 0):,}"
        
        return message
    
    async def send_daily_report(self):
        """Send single daily report and exit (for cron job)"""
        try:
            logging.info("Fetching status and generating daily report...")
            
            current_status = await self.fetch_status()
            
            if current_status is None:
                # API unavailable
                await self.bot.send_message("ğŸ”´ <b>Alert:</b> NewsBreak API is unavailable!")
                return
            
            # Generate and send daily report
            message = await self.format_daily_report(current_status)
            await self.bot.send_message(message)
            logging.info("Daily report sent successfully")
            
        except Exception as e:
            logging.error(f"Error sending daily report: {e}")
            error_msg = f"âŒ <b>Error:</b> Failed to generate daily report\n<code>{str(e)}</code>"
            try:
                await self.bot.send_message(error_msg)
            except:
                pass
            raise

async def main():
    # Configuration from environment variables
    BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
    CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")
    API_URL = os.getenv("NEWSBREAK_API_URL", "http://localhost:8000")
    DB_URL_DATA = os.getenv("DATABASE_URL_1", "postgresql://sanket:root@localhost:5432/newsbreak_scraper_data")
    DB_URL_URLS = os.getenv("DATABASE_URL_2", "postgresql://sanket:root@localhost:5432/newsbreak_scraper_urls")
    
    if not BOT_TOKEN or not CHAT_ID:
        print("Error: TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID environment variables are required")
        print("\nSetup Instructions:")
        print("1. Create a bot with @BotFather on Telegram")
        print("2. Get your chat ID by messaging @userinfobot")
        print("3. Set environment variables:")
        print("   export TELEGRAM_BOT_TOKEN='your_bot_token'")
        print("   export TELEGRAM_CHAT_ID='your_chat_id'")
        print("   export NEWSBREAK_API_URL='http://your-server:8000'  # optional")
        print("   export DATABASE_URL_1='postgresql://user:pass@host:port/newsbreak_scraper_data'  # optional")
        print("   export DATABASE_URL_2='postgresql://user:pass@host:port/newsbreak_scraper_urls'  # optional")
        return
    
    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('newsbreak-bot.log'),
            logging.StreamHandler()
        ]
    )
    
    # Initialize bot and monitor
    bot = TelegramBot(BOT_TOKEN, CHAT_ID)
    monitor = NewsBreakMonitor(API_URL, bot, DB_URL_DATA, DB_URL_URLS)
    
    try:
        # Send daily report and exit (for cron job)
        await monitor.send_daily_report()
        logging.info("Daily report completed successfully")
        
    except Exception as e:
        logging.error(f"Error: {e}")
        exit(1)

if __name__ == "__main__":
    asyncio.run(main())
